{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ee2ef1-1245-47e9-8cb9-5c2e3b0411dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4757,
     "status": "ok",
     "timestamp": 1715099931556,
     "user": {
      "displayName": "Vítězslav Lužný",
      "userId": "03315873559140428780"
     },
     "user_tz": -480
    },
    "id": "48ee2ef1-1245-47e9-8cb9-5c2e3b0411dd",
    "outputId": "ba29e326-5175-4eea-b822-aa3b28051eb5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import datetime\n",
    "\n",
    "### Load the dataset\n",
    "\n",
    "path_to_dataset = 'FinalData_reshaped.csv'\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    #CHANGE THIS\n",
    "    colab_path = \"/content/drive/MyDrive/Colab Notebooks/NTU_DA/\"\n",
    "    path_to_dataset = colab_path + path_to_dataset\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "df = pd.read_csv(path_to_dataset, parse_dates=True)\n",
    "\n",
    "timeSeriesData = np.genfromtxt('FinalData_reshaped_Y.csv', delimiter=',')\n",
    "station_cnt = timeSeriesData.shape[1]\n",
    "#We add an hour and day_in_a_week to the time series data\n",
    "timeSeriesData = np.concatenate([timeSeriesData, df['hour'].to_numpy()[:, None]], axis = 1)\n",
    "timeSeriesData = np.concatenate([timeSeriesData, df['day_in_a_week'].to_numpy()[:, None]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f89e90f-f6ff-412d-bd77-7af3bd37d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove incomplete rows\n",
    "incomplete_Y_rows = np.nonzero(np.isnan(timeSeriesData[:, 0]))[0]\n",
    "other_incomplete_rows = [(i+1) for i in incomplete_Y_rows if ((i+1) not in incomplete_Y_rows and (i+1) < timeSeriesData.shape[0])] #We have to exclude the following rows as well (since previous_mrt_flow features will be incomplete too)\n",
    "other_incomplete_rows += [0]\n",
    "complete_rows = [i for i in range(timeSeriesData.shape[0]) if (i not in incomplete_Y_rows and i not in other_incomplete_rows)]\n",
    "\n",
    "timeSeriesData_clean = timeSeriesData[complete_rows, :]\n",
    "df_clean = df.drop(incomplete_Y_rows.tolist() + other_incomplete_rows)\n",
    "df_clean = df_clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34beed7d-3145-4c36-b267-b544e74120d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3662,
     "status": "ok",
     "timestamp": 1715099937562,
     "user": {
      "displayName": "Vítězslav Lužný",
      "userId": "03315873559140428780"
     },
     "user_tz": -480
    },
    "id": "34beed7d-3145-4c36-b267-b544e74120d3",
    "outputId": "188f95f8-7ed9-4f19-da09-8bbf0963a062"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical indices:  [4, 3, 1, 2] \n",
      "numerical indices:  [0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135]\n",
      "\n",
      "\n",
      "24 unique hour:\t [1.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 0.0, 2.0, 3.0, 4.0]\n",
      "4 unique status:\t ['良好', '普通', '對敏感族群不健康', nan]\n",
      "7 unique day_in_a_week:\t [2.0, 3.0, 4.0, 5.0, 6.0, 0.0, 1.0]\n",
      "12 unique month:\t [6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 1.0, 2.0, 3.0, 4.0, 5.0]\n"
     ]
    }
   ],
   "source": [
    "#Dataset information\n",
    "categorical_variables = [\"hour\", \"status\", \"day_in_a_week\", \"month\"]\n",
    "\n",
    "cat_indices = [df_clean.columns.get_loc(categorical_variables[i]) for i in range(len(categorical_variables))]\n",
    "real_indices = [i for i in list(range(0, len(df_clean.columns))) if (i not in cat_indices)]\n",
    "print(\"categorical indices: \", cat_indices, \"\\nnumerical indices: \", real_indices)\n",
    "    \n",
    "unique_status        = list(dict.fromkeys([df_clean[\"status\"][i] for i in range(len(df_clean))]))\n",
    "print(\"\\n\")\n",
    "category_cnt = 0\n",
    "for var in categorical_variables:\n",
    "    unique_vals = list(dict.fromkeys([df_clean[var][i] for i in range(len(df_clean))]))\n",
    "    print(str(len(unique_vals)) + \" unique \" + var + \":\\t\", unique_vals)\n",
    "    category_cnt += len(unique_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69dc1f3f-9abb-45f1-a8a7-ea1e60156c05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3672,
     "status": "ok",
     "timestamp": 1715099943807,
     "user": {
      "displayName": "Vítězslav Lužný",
      "userId": "03315873559140428780"
     },
     "user_tz": -480
    },
    "id": "69dc1f3f-9abb-45f1-a8a7-ea1e60156c05",
    "outputId": "e8d61f62-a722-4f67-acdf-7bf7454cf9cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing station status\n",
      "\tFirst occurance of 良好: 0\n",
      "\tFirst occurance of 普通: 100\n",
      "\tFirst occurance of 對敏感族群不健康: 1996\n",
      "\n",
      "Train-val-test split\n",
      "\tTrain start index: 0\n",
      "\tVal start index: 6986\n",
      "\tTest start index: 7984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yplab\\anaconda3\\envs\\Vita_ML\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a, func=_nanmedian, keepdims=keepdims,\n",
      "C:\\Users\\yplab\\anaconda3\\envs\\Vita_ML\\Lib\\site-packages\\numpy\\lib\\nanfunctions.py:1384: RuntimeWarning: All-NaN slice encountered\n",
      "  return _nanquantile_unchecked(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6650, 336, 38)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.compose\n",
    "\n",
    "data = df_clean.to_numpy()\n",
    "\n",
    "print(\"\\nProcessing station status\")\n",
    "#Convert status names to ints\n",
    "for i in range(len(unique_status)):\n",
    "    occurances = np.nonzero(data == unique_status[i])[0]\n",
    "    if len(occurances) > 0:\n",
    "        print(\"\\tFirst occurance of \" + str(unique_status[i]) + \":\", occurances[0])\n",
    "    data[data == unique_status[i]] = i\n",
    "\n",
    "#Replace missing values with NaNs\n",
    "data[data == \"-\"] = np.nan\n",
    "\n",
    "##Train test split\n",
    "train_split_ratio = 0.7\n",
    "val_split_ratio = 0.1\n",
    "test_split_ratio = 0.2\n",
    "\n",
    "sample_cnt = data.shape[0]\n",
    "train_sz = int(train_split_ratio * sample_cnt)\n",
    "val_sz = int(val_split_ratio * sample_cnt)\n",
    "test_sz = int(test_split_ratio * sample_cnt)\n",
    "\n",
    "val_ind = train_sz + val_sz\n",
    "test_ind = val_ind + test_sz\n",
    "\n",
    "print(\"\\nTrain-val-test split\")\n",
    "print(\"\\tTrain start index:\", 0)\n",
    "print(\"\\tVal start index:\", train_sz)\n",
    "print(\"\\tTest start index:\", val_ind)\n",
    "\n",
    "timeSeries_train_raw = timeSeriesData_clean[:train_sz, :]\n",
    "X_train_raw = data[:train_sz, cat_indices + real_indices]\n",
    "\n",
    "timeSeries_val_raw = timeSeriesData_clean[train_sz:val_ind, :]\n",
    "X_val_raw = data[train_sz:val_ind, cat_indices + real_indices]\n",
    "\n",
    "timeSeries_test_raw = timeSeriesData_clean[val_ind:test_ind, :]\n",
    "X_test_raw = data[val_ind:test_ind, cat_indices + real_indices]\n",
    "\n",
    "##Preprocessing\n",
    "new_cat_indices = list(range(len(cat_indices)))\n",
    "new_real_indices = list(range(len(cat_indices), len(cat_indices) + len(real_indices)))\n",
    "\n",
    "#One hot encoding for categories, Robust scaling for numerical values\n",
    "X_preprocessor = sklearn.compose.ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", sklearn.preprocessing.OneHotEncoder(categories='auto', handle_unknown=\"ignore\", sparse_output=False), new_cat_indices),\n",
    "        (\"num\", sklearn.preprocessing.RobustScaler(), new_real_indices)\n",
    "    ]\n",
    ")\n",
    "X_preprocessor.fit(X_train_raw)\n",
    "X_train = X_preprocessor.transform(X_train_raw)\n",
    "X_val   = X_preprocessor.transform(X_val_raw)\n",
    "X_test  = X_preprocessor.transform(X_test_raw)\n",
    "\n",
    "X_train[np.isnan(X_train)] = 0\n",
    "X_val[np.isnan(X_val)] = 0\n",
    "X_test[np.isnan(X_test)] = 0\n",
    "\n",
    "#Scaling mrt flow\n",
    "\n",
    "Y_scaler = sklearn.preprocessing.RobustScaler().fit(timeSeries_train_raw[:, :-2])\n",
    "Y_train = Y_scaler.transform(timeSeries_train_raw[:, :-2])\n",
    "Y_val   = Y_scaler.transform(timeSeries_val_raw  [:, :-2])\n",
    "Y_test  = Y_scaler.transform(timeSeries_test_raw [:, :-2])\n",
    "\n",
    "#One-hot encoding an hour\n",
    "hour_onehot = sklearn.preprocessing.OneHotEncoder(categories='auto', handle_unknown=\"ignore\", sparse_output=False)\n",
    "hour_onehot.fit(timeSeries_train_raw[:, -2][:, None])\n",
    "timeSeries_train = np.concatenate([Y_train, hour_onehot.transform(timeSeries_train_raw[:, -2][:, None])], axis = 1)\n",
    "timeSeries_val   = np.concatenate([Y_val  , hour_onehot.transform(timeSeries_val_raw  [:, -2][:, None])], axis = 1)\n",
    "timeSeries_test  = np.concatenate([Y_test , hour_onehot.transform(timeSeries_test_raw [:, -2][:, None])], axis = 1)\n",
    "\n",
    "#One-hot encoding a day_in_a_week\n",
    "day_onehot = sklearn.preprocessing.OneHotEncoder(categories='auto', handle_unknown=\"ignore\", sparse_output=False)\n",
    "day_onehot.fit(timeSeries_train_raw[:, -1][:, None])\n",
    "timeSeries_train = np.concatenate([timeSeries_train, day_onehot.transform(timeSeries_train_raw[:, -1][:, None])], axis = 1)\n",
    "timeSeries_val   = np.concatenate([timeSeries_val  , day_onehot.transform(timeSeries_val_raw  [:, -1][:, None])], axis = 1)\n",
    "timeSeries_test  = np.concatenate([timeSeries_test , day_onehot.transform(timeSeries_test_raw [:, -1][:, None])], axis = 1)\n",
    "\n",
    "def Y_inverse_transform(Y):\n",
    "    return Y_scaler.inverse_transform(Y)\n",
    "\n",
    "#Turn time series into sequences\n",
    "sequence_length = 168*2\n",
    "def timeSeriesReshape(timeSeries):\n",
    "    reshapedTimeSeries = np.zeros((timeSeries.shape[0] - sequence_length, sequence_length, timeSeries.shape[1]))\n",
    "    for i in range(sequence_length, timeSeries.shape[0]):\n",
    "        reshapedTimeSeries[i-sequence_length, :, :] = timeSeries[i-sequence_length:i, :]\n",
    "    return reshapedTimeSeries\n",
    "\n",
    "Y_train = Y_train[sequence_length:, :]\n",
    "Y_val   = Y_val  [sequence_length:, :]\n",
    "Y_test  = Y_test [sequence_length:, :]\n",
    "\n",
    "X_train = X_train[sequence_length:, :]\n",
    "X_val   = X_val  [sequence_length:, :]\n",
    "X_test  = X_test [sequence_length:, :]\n",
    "\n",
    "timeSeries_train = timeSeriesReshape(timeSeries_train)\n",
    "timeSeries_val   = timeSeriesReshape(timeSeries_val  )\n",
    "timeSeries_test  = timeSeriesReshape(timeSeries_test )\n",
    "\n",
    "print(timeSeries_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4879e0-4073-441b-8f66-a3117dda2f00",
   "metadata": {
    "id": "bb4879e0-4073-441b-8f66-a3117dda2f00"
   },
   "source": [
    "# PyTorch initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1ca8b37-8e58-4501-98f7-e40c489f5af7",
   "metadata": {
    "id": "a1ca8b37-8e58-4501-98f7-e40c489f5af7",
    "outputId": "d7d61b49-8b1c-4ece-c6f8-fa766bee33b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  True\n",
      "Using Cuda:  True\n",
      "NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import copy\n",
    "import time\n",
    "\n",
    "default_dtype = torch.float32\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "#Initialize PyTorch\n",
    "USE_CUDA = True\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"Cuda available: \", cuda_available)\n",
    "print(\"Using Cuda: \", USE_CUDA)\n",
    "\n",
    "if cuda_available:\n",
    "  # print(torch.cuda.current_device())\n",
    "  # print(torch.cuda.device_count())\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "  cuda_device = torch.device(\"cuda:0\")\n",
    "\n",
    "#Move the dataset to GPU\n",
    "input_size = X_train.shape[1]\n",
    "timeSeries_input_size = timeSeries_train.shape[2]\n",
    "output_size = station_cnt\n",
    "\n",
    "X_train_t          = torch.from_numpy(X_train).to(dtype = default_dtype)\n",
    "X_val_t            = torch.from_numpy(X_val  ).to(dtype = default_dtype)\n",
    "X_test_t           = torch.from_numpy(X_test ).to(dtype = default_dtype)\n",
    "Y_train_t          = torch.from_numpy(Y_train).to(dtype = default_dtype)\n",
    "Y_val_t            = torch.from_numpy(Y_val  ).to(dtype = default_dtype)\n",
    "Y_test_t           = torch.from_numpy(Y_test ).to(dtype = default_dtype)\n",
    "timeSeries_train_t = torch.from_numpy(timeSeries_train).to(dtype = default_dtype)\n",
    "timeSeries_val_t   = torch.from_numpy(timeSeries_val  ).to(dtype = default_dtype)\n",
    "timeSeries_test_t  = torch.from_numpy(timeSeries_test ).to(dtype = default_dtype)\n",
    "\n",
    "if USE_CUDA and cuda_available:\n",
    "    X_train_t          = X_train_t.cuda()\n",
    "    X_val_t            = X_val_t.cuda()\n",
    "    X_test_t           = X_test_t.cuda()\n",
    "    Y_train_t          = Y_train_t.cuda()\n",
    "    Y_val_t            = Y_val_t.cuda()\n",
    "    Y_test_t           = Y_test_t.cuda()\n",
    "    timeSeries_train_t = timeSeries_train_t.cuda()\n",
    "    timeSeries_val_t   = timeSeries_val_t.cuda()\n",
    "    timeSeries_test_t  = timeSeries_test_t.cuda()\n",
    "\n",
    "loss_functions = {\n",
    "    'SmoothL1': F.smooth_l1_loss,\n",
    "    'L1': F.l1_loss,\n",
    "    'MSE': F.mse_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b79b1-47dc-4079-83e7-a540655ddc67",
   "metadata": {
    "id": "a14b79b1-47dc-4079-83e7-a540655ddc67"
   },
   "source": [
    "# PyTorch training implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e77347-3df8-4c2f-8aea-ba31319f0212",
   "metadata": {
    "id": "10e77347-3df8-4c2f-8aea-ba31319f0212"
   },
   "outputs": [],
   "source": [
    "##Initialize model weights\n",
    "def weights_init(layer_in):\n",
    "  if isinstance(layer_in, nn.Linear):\n",
    "    torch.nn.init.xavier_uniform_(layer_in.weight, gain=0.5)\n",
    "    layer_in.bias.data.fill_(0.0)\n",
    "\n",
    "## Turn data into sequences that can be used for training\n",
    "def get_batch_sequences(batch_idx, batch_size, permutation, X, timeSeries, Y):\n",
    "    sample_sz = X.shape[0]\n",
    "\n",
    "    batch_start = batch_idx * batch_size\n",
    "    batch_end   = min(sample_sz, (batch_idx + 1) * batch_size)\n",
    "    actual_size = batch_end - batch_start\n",
    "\n",
    "    batch_indeces = permutation[batch_start:batch_end]\n",
    "    \n",
    "    batch_target = Y[batch_indeces]\n",
    "    batch_series = timeSeries[batch_indeces]\n",
    "    batch_data = X[batch_indeces]\n",
    "\n",
    "    return [batch_data, batch_series, batch_target, actual_size]\n",
    "\n",
    "## Training step\n",
    "def train(epoch, model, optimizer, scheduler, X, timeSeries, Y, batch_size, loss_fnc, print_mode, schedule):\n",
    "    model.train()\n",
    "\n",
    "    sample_sz = X.shape[0]\n",
    "    perm = np.random.permutation(sample_sz)\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch_idx in range(0, int(np.ceil(sample_sz / batch_size))):\n",
    "        batch_data, batch_series, batch_target, actual_size = get_batch_sequences(batch_idx, batch_size, perm, X, timeSeries, Y)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model([batch_data, batch_series])\n",
    "        loss = loss_fnc(output, batch_target, reduction=\"mean\")\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if schedule:\n",
    "            scheduler.step()\n",
    "        if print_mode:\n",
    "            print('Epoch:{} \\tTrain loss: {:.7f}'.format(epoch, total_loss / (batch_idx + 1)), end=\"\\r\")\n",
    "\n",
    "    total_loss /= np.ceil(sample_sz / batch_size)\n",
    "    if not print_mode:\n",
    "        print('Epoch:{} \\tTrain loss: {:.7f}'.format(epoch, total_loss), end=\"\\r\")\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "## Testing\n",
    "def test(model, X, timeSeries, Y, batch_size, loss_fnc):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    sample_sz = X.shape[0]\n",
    "    with torch.no_grad():\n",
    "        perm = np.random.permutation(sample_sz)\n",
    "\n",
    "        for batch_idx in range(0, int(np.ceil(sample_sz / batch_size))):\n",
    "            batch_data, batch_series, batch_target, actual_size = get_batch_sequences(batch_idx, batch_size, perm, X, timeSeries, Y)\n",
    "\n",
    "            output = model([batch_data, batch_series])\n",
    "            test_loss += loss_fnc(output, batch_target, reduction=\"mean\").item()\n",
    "\n",
    "    test_loss /= np.ceil(sample_sz / batch_size)\n",
    "    return test_loss\n",
    "\n",
    "def predict(model, X, timeSeries, batch_size):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    sample_sz = X.shape[0]\n",
    "    Y_dummy = np.zeros((sample_sz, output_size))\n",
    "    Y_hat = np.zeros((sample_sz, output_size))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        perm = np.arange(0, sample_sz)\n",
    "\n",
    "        for batch_idx in range(0, int(np.ceil(sample_sz / batch_size))):\n",
    "            batch_data, batch_series, batch_target, actual_size = get_batch_sequences(batch_idx, batch_size, perm, X, timeSeries, Y_dummy)\n",
    "\n",
    "            output = model([batch_data, batch_series])\n",
    "\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + actual_size\n",
    "\n",
    "            Y_hat[start_idx:end_idx] = output[:].cpu()\n",
    "\n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c34b9c-5455-4274-9597-1a8514a84c2a",
   "metadata": {
    "id": "03c34b9c-5455-4274-9597-1a8514a84c2a"
   },
   "source": [
    "# Training and Grid search procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d59fb3-8161-41b0-83a7-f3977e7bbd69",
   "metadata": {
    "id": "f3d59fb3-8161-41b0-83a7-f3977e7bbd69"
   },
   "outputs": [],
   "source": [
    "\n",
    "## The whole training\n",
    "def train_model(model_class, params, max_batch_size, print_mode):\n",
    "    model = model_class(params)\n",
    "    loss_fnc = loss_functions[params[\"loss\"]]\n",
    "    epochs = params[\"epochs\"]\n",
    "    lr = params[\"lr\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    weight_decay = params[\"weight_decay\"]\n",
    "    batch_growth = params[\"batch_growth\"]\n",
    "\n",
    "\n",
    "    if USE_CUDA and cuda_available:\n",
    "      model = model.cuda()\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Get initial performance\n",
    "    test_loss = test(model, X_val_t, timeSeries_val_t, Y_val_t, batch_size, loss_functions[params[\"loss\"]])\n",
    "    if print_mode:\n",
    "        print('\\nInitial: Test loss: {:.7f}\\n'.format(test_loss))\n",
    "\n",
    "    # Train\n",
    "    train_loss = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    scheduler_steps = int(epochs * np.ceil(X_train_t.shape[0] / batch_size))\n",
    "    if batch_growth:\n",
    "        scheduler_steps = int((epochs // 2) * np.ceil(X_train_t.shape[0] / max_batch_size))\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=scheduler_steps, eta_min=0) #Cosine decay\n",
    "\n",
    "    batch_increase_range = epochs - epochs // 2\n",
    "    batch_increase_step_sz = batch_increase_range / np.log2(max_batch_size / batch_size)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        schedule = (epoch > np.ceil(batch_increase_range)) or (not batch_growth)\n",
    "        current_batch_size = batch_size\n",
    "        if batch_growth:\n",
    "            current_batch_size = np.round(batch_size * np.power(2, (epoch - 1) // batch_increase_step_sz))\n",
    "            current_batch_size = int(min(current_batch_size, max_batch_size))\n",
    "        train_loss = train(epoch, model, optimizer, scheduler, X_train_t, timeSeries_train_t, Y_train_t, current_batch_size, loss_fnc, print_mode, schedule)\n",
    "\n",
    "        if print_mode:\n",
    "            # train_loss = test(model, X_train_t, timeSeries_train_t, Y_train_t, batch_size, loss_fnc)\n",
    "            val_loss   = test(model, X_val_t, timeSeries_val_t, Y_val_t, batch_size, loss_fnc)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            # print('Epoch:' + str(epoch) + ' \\tbatch size: {} \\t LR: {}'.format(current_batch_size, optimizer.param_groups[0]['lr']))\n",
    "            print('Epoch:' + str(epoch) + ' \\tTrain loss: {:.7f} \\tVal loss: {:.7f}'.format(train_loss, val_loss) + ' \\tbatch size: {} \\t LR: {}'.format(current_batch_size, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    final_val_loss = test(model, X_val_t, timeSeries_val_t, Y_val_t, batch_size, loss_fnc)\n",
    "    print(\"\\nTime to train: {:.3f}s\\t final train loss:{:.7f}\\t final val loss:{:.7f}\".format(time.time() - start_time, train_loss, final_val_loss))\n",
    "\n",
    "    if print_mode:\n",
    "        return [model, train_losses, val_losses]\n",
    "    else:\n",
    "        return [model, train_loss, final_val_loss]\n",
    "\n",
    "## Grid search\n",
    "def grid_search(model_class, param_variations, max_batch_size):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_train_loss = float(\"inf\")\n",
    "    best_val_params = None\n",
    "    best_train_params = None\n",
    "\n",
    "    val_losses = []\n",
    "    train_losses = []\n",
    "\n",
    "    for i in range(len(param_variations)):\n",
    "        params = param_variations[i]\n",
    "        print(\"\\nGrid search step {}/{}\".format(i + 1, len(param_variations)), \"\\t| params:\", params)\n",
    "\n",
    "        model, train_loss, val_loss = train_model(model_class, params, max_batch_size, print_mode=False)\n",
    "\n",
    "        if best_train_loss > train_loss:\n",
    "            print(\"New train best!\")\n",
    "            best_train_loss = train_loss\n",
    "            best_train_params = params\n",
    "            \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if best_val_loss > val_loss:\n",
    "            print(\"New val best!\")\n",
    "            best_val_loss = val_loss\n",
    "            best_val_params = params\n",
    "            \n",
    "\n",
    "    print(\"\\nBest validation loss: \", best_val_loss)\n",
    "    print(\"\\nBest training loss: \", best_train_loss)\n",
    "    print(\"Best val params: \", best_val_params)\n",
    "    print(\"Best train params: \", best_train_params)\n",
    "    # model, train_losses, val_losses = train_model(model_class, best_params, print_mode=True)\n",
    "    return [best_train_params, best_val_params, train_losses, val_losses]\n",
    "\n",
    "## Create list of parameter dictionaries for grid search\n",
    "def create_param_dict_from_lists(list_of_params, list_of_names):\n",
    "    params = []\n",
    "\n",
    "    feature_vals = list_of_params[0]\n",
    "    feature_name = list_of_names[0]\n",
    "    for feature_val in feature_vals:\n",
    "        params_row = {}\n",
    "        params_row[feature_name] = feature_val\n",
    "        params.append(params_row)\n",
    "\n",
    "    for i in range(1, len(list_of_params)):\n",
    "        feature_vals = list_of_params[i]\n",
    "        feature_name = list_of_names[i]\n",
    "\n",
    "        old_params = params\n",
    "        new_params = []\n",
    "        for past_param in old_params:\n",
    "            for feature_val in feature_vals:\n",
    "                params_row = copy.copy(past_param)\n",
    "                params_row[feature_name] = feature_val\n",
    "                new_params.append(params_row)\n",
    "        params = new_params\n",
    "\n",
    "    return params\n",
    "\n",
    "## Grid search plotting\n",
    "def plot_grid(grid_values, title, x_label, y_label, x_values, y_values):\n",
    "    plt.matshow(grid_values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(list(range(len(x_values))), x_values)\n",
    "    plt.yticks(list(range(len(y_values))), y_values)\n",
    "\n",
    "    for (i, j), z in np.ndenumerate(grid_values):\n",
    "        plt.text(j, i, '{:0.5f}'.format(z), ha='center', va='center')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "## Plot training progress\n",
    "def plot_losses(train_losses, val_losses, title):\n",
    "    plt.title(title)\n",
    "    plt.plot(train_losses, label=\"train loss\")\n",
    "    plt.plot(val_losses, label=\"val loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "## Evaluation\n",
    "def evaluate_model(model, batch_size):\n",
    "    def print_metrics(model, X, timeSeries, Y, batch_size):\n",
    "        Y_hat = predict(model, X, timeSeries, batch_size)\n",
    "\n",
    "        MSE = np.mean((Y_hat - Y) ** 2)\n",
    "        orig_MSE = np.mean((Y_inverse_transform(Y_hat) - Y_inverse_transform(Y)) ** 2)\n",
    "        print(\"\\tMSE:\", MSE)\n",
    "        print(\"\\tOriginal scale MSE:\", orig_MSE)\n",
    "\n",
    "        print(\"\\n\\tRMSE:\", np.sqrt(MSE))\n",
    "        print(\"\\tOriginal scale RMSE:\", np.sqrt(orig_MSE))\n",
    "\n",
    "        print(\"\\n\\tMAE:\", np.mean(np.abs(Y_hat - Y)))\n",
    "        print(\"\\tOriginal scale MAE:\", np.mean(np.abs(Y_inverse_transform(Y_hat) - Y_inverse_transform(Y))))\n",
    "\n",
    "    print(\"Train metrics:\")\n",
    "    print_metrics(model, X_train_t, timeSeries_train_t, Y_train, batch_size)\n",
    "    print(\"\\nValidation metrics:\")\n",
    "    print_metrics(model, X_val_t, timeSeries_val_t, Y_val, batch_size)\n",
    "    print(\"\\nTest metrics:\")\n",
    "    print_metrics(model, X_test_t, timeSeries_test_t, Y_test, batch_size)\n",
    "    \n",
    "## Learning Rate Range Test\n",
    "def Learning_Rate_Range_Test(model_class, min_lr, max_lr, epochs, params):\n",
    "    model = model_class(params)\n",
    "    loss_fnc = loss_functions[params[\"loss\"]]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    weight_decay = params[\"weight_decay\"]\n",
    "\n",
    "    step_cnt = epochs\n",
    "    gamma = np.power((max_lr/min_lr), 1/step_cnt)\n",
    "    print(\"gamma per epoch:\", gamma)\n",
    "    print(\"residual:\", np.abs(max_lr/min_lr - np.power(gamma, step_cnt)))\n",
    "\n",
    "    if USE_CUDA and cuda_available:\n",
    "      model = model.cuda()\n",
    "    model.apply(weights_init)\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Get initial performance\n",
    "    test_loss = test(model, X_train_t, timeSeries_train_t, Y_train_t, batch_size, loss_functions[params[\"loss\"]])\n",
    "    print('\\nInitial: Train loss: {:.7f}\\n'.format(test_loss))\n",
    "\n",
    "    # Train\n",
    "    train_loss = None\n",
    "    train_losses = []\n",
    "    learning_rates = []\n",
    "\n",
    "    # scheduler = lr_scheduler.ExponentialLR(optimizer, gamma = gamma)\n",
    "    lr=min_lr\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model_copy = model_class(params)        \n",
    "        if USE_CUDA and cuda_available:\n",
    "          model_copy = model_copy.cuda()\n",
    "        model_copy.load_state_dict(model.state_dict())\n",
    "        optimizer = optim.Adam(model_copy.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        train_loss = train(epoch, model_copy, optimizer, None, X_train_t, timeSeries_train_t, Y_train_t, batch_size, loss_fnc, True, False)\n",
    "        # learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        learning_rates.append(lr)\n",
    "        train_losses.append(train_loss)\n",
    "        print('Step:' + str(epoch) + ' \\tTrain loss: {:.7f} \\tLR:'.format(train_loss), learning_rates[-1])\n",
    "        lr*=gamma\n",
    "\n",
    "    print(\"\\nTime to train: {:.3f}s\".format(time.time() - start_time))\n",
    "\n",
    "    train_losses = np.array(train_losses)\n",
    "    learning_rates = np.array(learning_rates)\n",
    "    min_loss_lr = learning_rates[np.argmin(train_losses)]\n",
    "    \n",
    "    plt.plot(learning_rates, train_losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.gca().set_yscale('log')\n",
    "    plt.xlabel(\"learning rate\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()\n",
    "    \n",
    "    loss_diff = train_losses[1:]/train_losses[:-1]\n",
    "    max_drop_loss_lr = learning_rates[np.argmin(loss_diff) + 1]\n",
    "    \n",
    "    # plt.plot(learning_rates[1:], loss_diff)\n",
    "    # plt.gca().set_xscale('log')\n",
    "    # plt.gca().set_yscale('log')\n",
    "    # plt.xlabel(\"learning rate\")\n",
    "    # plt.ylabel(\"loss differential\")\n",
    "    # plt.show()\n",
    "\n",
    "    print(\"min_loss_lr\", min_loss_lr)\n",
    "    print(\"max_drop_loss_lr\", max_drop_loss_lr)\n",
    "    # best_lr = max_drop_loss_lr\n",
    "    \n",
    "    best_lr = min_loss_lr\n",
    "\n",
    "    return [model, train_losses, learning_rates, best_lr]\n",
    "\n",
    "## Get maximum batch size\n",
    "def get_max_batch_size(model):\n",
    "    max_batch_size = 2\n",
    "    Y_train_hat = predict(model, X_train_t, timeSeries_train_t, max_batch_size)\n",
    "    try:\n",
    "        while max_batch_size < X_train_t.shape[0]:\n",
    "            Y_train_hat = predict(model, X_train_t, timeSeries_train_t, max_batch_size)\n",
    "            max_batch_size *= 2\n",
    "        print(\"batch size of {} is larger than the dataset size!\".format(max_batch_size))\n",
    "    except:\n",
    "        print(\"batch size of {} caused OOM!\".format(max_batch_size))\n",
    "    \n",
    "    max_batch_size = int(max_batch_size / 4)\n",
    "    print(\"max_batch_size:\", max_batch_size)\n",
    "    \n",
    "    return max_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45ab82-509e-4b89-98a4-7c3c062bc5ba",
   "metadata": {},
   "source": [
    "# PyTorch Transformer with embedding implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d171baee-7585-425f-bf98-e7c1bf8cbc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_model(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Transformer_model, self).__init__()\n",
    "\n",
    "        hidden_size = params['hidden_size']\n",
    "        num_layers = params['num_layers']\n",
    "        num_heads = params['num_heads']\n",
    "\n",
    "        self.embedding_fc = nn.Linear(timeSeries_input_size, hidden_size)\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        \n",
    "        self.hidden_fc = nn.Linear(hidden_size + input_size, hidden_size * 4)\n",
    "        self.fc = nn.Linear(hidden_size * 4, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        X = x[0]\n",
    "        timeSeries = x[1]\n",
    "\n",
    "        out = self.embedding_fc(timeSeries)\n",
    "        out = self.transformer_encoder(out)\n",
    "        out = out.mean(1)\n",
    "        out = torch.cat([out, X], dim=1)\n",
    "        out = self.hidden_fc(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc86dc87-fae1-486e-9983-4e458601c496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma per epoch: 1.9952623149688797\n",
      "residual: 7.958078640513122e-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yplab\\anaconda3\\envs\\Vita_ML\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:720: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  return torch._transformer_encoder_layer_fwd(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial: Train loss: 1.1016022\n",
      "\n",
      "Step:1 \tTrain loss: 0.5410399 \tLR: 1e-05\n",
      "Step:2 \tTrain loss: 0.4131122 \tLR: 1.99526231496888e-05\n",
      "Step:3 \tTrain loss: 0.3014049 \tLR: 3.9810717055349735e-05\n",
      "Step:4 \tTrain loss: 0.2339433 \tLR: 7.943282347242818e-05\n",
      "Step:5 \tTrain loss: 0.1920147 \tLR: 0.00015848931924611142\n",
      "Step:6 \tTrain loss: 0.1967007 \tLR: 0.0003162277660168381\n",
      "Step:7 \tTrain loss: 0.2391229 \tLR: 0.0006309573444801936\n",
      "Step:8 \tTrain loss: 0.2351539 \tLR: 0.0012589254117941681\n",
      "Step:9 \tTrain loss: 0.3262189 \tLR: 0.002511886431509582\n",
      "Step:10 \tTrain loss: 3.4702028 \tLR: 0.005011872336272727\n",
      "\n",
      "Time to train: 300.843s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_loss_lr 0.00015848931924611142\n",
      "max_drop_loss_lr 3.9810717055349735e-05\n",
      "best_lr: 0.00015848931924611142\n",
      "batch size of 2048 caused OOM!\n",
      "max_batch_size: 512\n",
      "max_batch_size_TF: 128\n"
     ]
    }
   ],
   "source": [
    "# Find the best initial learning rate\n",
    "hidden_size = 128\n",
    "num_layers = 8\n",
    "num_heads = 16\n",
    "\n",
    "params = {'batch_size': 4, 'loss': 'MSE', 'weight_decay': 0, 'hidden_size':hidden_size, 'num_layers':num_layers, 'num_heads':num_heads}\n",
    "min_lr = 1e-5\n",
    "max_lr = 1e-2\n",
    "steps = 10\n",
    "\n",
    "model, train_losses, learning_rates, best_lr_TF = Learning_Rate_Range_Test(Transformer_model, min_lr, max_lr, steps, params)\n",
    "print(\"best_lr:\", best_lr_TF)\n",
    "\n",
    "# Find the maximum batch size\n",
    "max_batch_size_TF = get_max_batch_size(model) // 4\n",
    "print(\"max_batch_size_TF:\", max_batch_size_TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88192d-149f-4522-8fb1-91c8ebbb3d39",
   "metadata": {},
   "source": [
    "# PyTorch Transformer with embedding training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323a768-385b-40ad-a483-90831be68a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All grid searched values:\n",
      "\t {'epochs': 100, 'batch_size': 4, 'lr': 0.00015848931924611142, 'batch_growth': True, 'loss': 'MSE', 'weight_decay': 0, 'dropout_rate': None, 'hidden_size': 128, 'num_layers': 8, 'num_heads': 16}\n",
      "\n",
      "Grid search step 1/1 \t| params: {'epochs': 100, 'batch_size': 4, 'lr': 0.00015848931924611142, 'batch_growth': True, 'loss': 'MSE', 'weight_decay': 0, 'dropout_rate': None, 'hidden_size': 128, 'num_layers': 8, 'num_heads': 16}\n",
      "Epoch:57 \tTrain loss: 0.0643478\r"
     ]
    }
   ],
   "source": [
    "# Parameters for grid search\n",
    "epochs = [100]\n",
    "batch_size = [4]\n",
    "dropout_rate = [None]\n",
    "lrs = [best_lr_TF]\n",
    "weight_decay = [0]\n",
    "batch_growths = [True]\n",
    "loss = [\"MSE\"]\n",
    "\n",
    "hidden_sizes = [hidden_size]\n",
    "num_layerss = [num_layers]\n",
    "num_headss = [num_heads]\n",
    "\n",
    "\n",
    "# Parameter formatting\n",
    "all_param_vals = [epochs, batch_size, lrs, batch_growths, loss, weight_decay, dropout_rate, hidden_sizes, num_layerss, num_headss]\n",
    "feature_names = [\"epochs\", \"batch_size\", \"lr\", \"batch_growth\", \"loss\", \"weight_decay\", \"dropout_rate\", \"hidden_size\", \"num_layers\", \"num_heads\"]\n",
    "params = create_param_dict_from_lists(all_param_vals, feature_names)\n",
    "\n",
    "print(\"All grid searched values:\")\n",
    "for param in params:\n",
    "    print(\"\\t\", param)\n",
    "\n",
    "# Grid search\n",
    "best_train_params_TF, best_val_params_TF, grid_train_losses_TF, grid_val_losses_TF = grid_search(Transformer_model, params, max_batch_size_TF)\n",
    "\n",
    "print(\"Final training for the min val loss:\")\n",
    "model_TF, train_losses_TF, val_losses_TF = train_model(Transformer_model, best_val_params_TF, max_batch_size_TF, print_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0655f52-284b-43c2-baf4-ed8ebafa8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show grid search progress\n",
    "train_grid = np.zeros((len(epochs), len(batch_growths)))\n",
    "val_grid = np.zeros((len(epochs), len(batch_growths)))\n",
    "\n",
    "for i in range(len(grid_val_losses_TF)):\n",
    "    train_grid[i // len(batch_growths), i % len(batch_growths)] = grid_train_losses_TF[i]\n",
    "    val_grid  [i // len(batch_growths), i % len(batch_growths)] = grid_val_losses_TF[i]\n",
    "\n",
    "plot_grid(grid_values=train_grid, title=\"TF MSE Grid search train losses\", x_label=\"batch_growth\", y_label=\"epochs\", x_values=batch_growths, y_values=epochs)\n",
    "plot_grid(grid_values=val_grid  , title=\"TF MSE Grid search val losses\"  , x_label=\"batch_growth\", y_label=\"epochs\", x_values=batch_growths, y_values=epochs)\n",
    "\n",
    "# Plot the training progress\n",
    "plot_losses(train_losses_TF, val_losses_TF, \"Transformer\")\n",
    "\n",
    "# Show final model evaluation\n",
    "print(\"best train params: \", best_train_params_TF)\n",
    "print(\"best val params: \", best_val_params_TF)\n",
    "evaluate_model(model_TF, max_batch_size_TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd386009-c59d-4047-968b-f38a18f585ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the final model\n",
    "print(\"Saving model...\")\n",
    "path_to_save = \"Transformer\"\n",
    "torch.save(model_TF, path_to_save)\n",
    "\n",
    "with open('Transformer_params.txt', 'w') as f:\n",
    "    print(best_val_params_TF, file=f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
