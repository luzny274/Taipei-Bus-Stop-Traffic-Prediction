{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ee2ef1-1245-47e9-8cb9-5c2e3b0411dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Load the dataset\n",
    "\n",
    "path_to_dataset = 'FinalData.csv'\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    #CHANGE THIS\n",
    "    colab_path = \"/content/drive/MyDrive/Colab Notebooks/NTU_DA/\"\n",
    "    path_to_dataset = colab_path + path_to_dataset\n",
    "    \n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "df = pd.read_csv(path_to_dataset, parse_dates=True)\n",
    "df = df.drop('datetime', axis=1) #We remove the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34beed7d-3145-4c36-b267-b544e74120d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metro_flow index:  1 \n",
      "categorical indices:  [0, 2, 3, 15] \n",
      "numerical indices:  [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "\n",
      "unique_station_names: ['中山', '北投', '北門', '古亭', '士林', '大橋頭', '松山']\n",
      "unique_status: ['良好', '普通', '對敏感族群不健康']\n"
     ]
    }
   ],
   "source": [
    "#Dataset information\n",
    "categorical_variables = [\"metro_station\", \"hour\", \"day\", \"status\"]\n",
    "\n",
    "y_ind = df.columns.get_loc(\"metro_flow\")\n",
    "cat_indices = [df.columns.get_loc(categorical_variables[i]) for i in range(len(categorical_variables))]\n",
    "real_indices = [i for i in list(range(0, len(df.columns))) if (i not in cat_indices and i != y_ind)]\n",
    "print(\"metro_flow index: \", y_ind, \"\\ncategorical indices: \", cat_indices, \"\\nnumerical indices: \", real_indices)\n",
    "\n",
    "unique_station_names = list(dict.fromkeys([df[\"metro_station\"][i] for i in range(len(df))]))\n",
    "print(\"\\nunique_station_names:\", unique_station_names)\n",
    "\n",
    "unique_status = list(dict.fromkeys([df[\"status\"][i] for i in range(len(df))]))\n",
    "print(\"unique_status:\", unique_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69dc1f3f-9abb-45f1-a8a7-ea1e60156c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing station names\n",
      "\tFirst occurance of 中山 0\n",
      "\tFirst occurance of 北投 1\n",
      "\tFirst occurance of 北門 2\n",
      "\tFirst occurance of 古亭 3\n",
      "\tFirst occurance of 士林 4\n",
      "\tFirst occurance of 大橋頭 5\n",
      "\tFirst occurance of 松山 6\n",
      "\n",
      "Processing station status\n",
      "\tFirst occurance of 良好 0\n",
      "\tFirst occurance of 普通 40\n",
      "\tFirst occurance of 對敏感族群不健康 3198\n",
      "\n",
      "Train-val-test split\n",
      "\tTrain start index: 0\n",
      "\tVal start index: 3309\n",
      "\tTest start index: 3722\n"
     ]
    }
   ],
   "source": [
    "import sklearn.model_selection\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.compose\n",
    "import sklearn.linear_model\n",
    "\n",
    "data = df.to_numpy()\n",
    "\n",
    "\n",
    "#Convert station names to ints\n",
    "print(\"Processing station names\")\n",
    "for i in range(len(unique_station_names)):\n",
    "    print(\"\\tFirst occurance of \" + unique_station_names[i] + \":\", np.nonzero(data == unique_station_names[i])[0][0])\n",
    "    data[data == unique_station_names[i]] = i\n",
    "\n",
    "print(\"\\nProcessing station status\")\n",
    "#Convert status names to ints\n",
    "for i in range(len(unique_status)):\n",
    "    print(\"\\tFirst occurance of \" + unique_status[i] + \":\", np.nonzero(data == unique_status[i])[0][0])\n",
    "    data[data == unique_status[i]] = i\n",
    "\n",
    "#Replace missing values with NaNs\n",
    "data[data == \"-\"] = np.nan\n",
    "\n",
    "##Train test split\n",
    "train_split_ratio = 0.8\n",
    "val_split_ratio = 0.1\n",
    "test_split_ratio = 0.1\n",
    "\n",
    "sample_cnt = data.shape[0]\n",
    "train_sz = int(train_split_ratio * sample_cnt)\n",
    "val_sz = int(val_split_ratio * sample_cnt)\n",
    "test_sz = int(test_split_ratio * sample_cnt)\n",
    "\n",
    "val_ind = train_sz + val_sz\n",
    "test_ind = val_ind + test_sz\n",
    "\n",
    "print(\"\\nTrain-val-test split\")\n",
    "print(\"\\tTrain start index:\", 0)\n",
    "print(\"\\tVal start index:\", train_sz)\n",
    "print(\"\\tTest start index:\", val_ind)\n",
    "\n",
    "Y_train_raw = data[:train_sz, y_ind][:, None]\n",
    "X_train_raw = data[:train_sz, cat_indices + real_indices]\n",
    "\n",
    "Y_val_raw = data[train_sz:val_ind, y_ind][:, None]\n",
    "X_val_raw = data[train_sz:val_ind, cat_indices + real_indices]\n",
    "\n",
    "Y_test_raw = data[val_ind:test_ind, y_ind][:, None]\n",
    "X_test_raw = data[val_ind:test_ind, cat_indices + real_indices]\n",
    "\n",
    "##Preprocessing\n",
    "new_cat_indices = list(range(len(cat_indices)))\n",
    "new_real_indices = list(range(len(cat_indices), len(cat_indices) + len(real_indices)))\n",
    "\n",
    "#One hot encoding for categories, Robust scaling for numerical values\n",
    "X_preprocessor = sklearn.compose.ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", sklearn.preprocessing.OneHotEncoder(categories='auto', handle_unknown=\"ignore\"), new_cat_indices),\n",
    "        (\"num\", sklearn.preprocessing.RobustScaler(), new_real_indices)\n",
    "    ]\n",
    ")\n",
    "X_preprocessor.fit(X_train_raw)\n",
    "X_train = X_preprocessor.transform(X_train_raw)\n",
    "X_val   = X_preprocessor.transform(X_val_raw)\n",
    "X_test  = X_preprocessor.transform(X_test_raw)\n",
    "\n",
    "#Robust scaling for Y\n",
    "Y_scaler = sklearn.preprocessing.RobustScaler().fit(Y_train_raw)\n",
    "Y_train = Y_scaler.transform(Y_train_raw)\n",
    "Y_val   = Y_scaler.transform(Y_val_raw)\n",
    "Y_test  = Y_scaler.transform(Y_test_raw)\n",
    "\n",
    "#Replace NaNs with zeros\n",
    "X_train[np.isnan(X_train)] = 0\n",
    "X_val[np.isnan(X_val)] = 0\n",
    "X_test[np.isnan(X_test)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4879e0-4073-441b-8f66-a3117dda2f00",
   "metadata": {},
   "source": [
    "# PyTorch initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1ca8b37-8e58-4501-98f7-e40c489f5af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available:  True\n",
      "Using Cuda:  False\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import copy\n",
    "import time\n",
    "\n",
    "default_dtype = torch.float32\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "#Initialize PyTorch\n",
    "USE_CUDA = True\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"Cuda available: \", cuda_available)\n",
    "print(\"Using Cuda: \", USE_CUDA)\n",
    "\n",
    "if cuda_available:\n",
    "  # print(torch.cuda.current_device())\n",
    "  # print(torch.cuda.device_count())\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "  cuda_device = torch.device(\"cuda:0\")\n",
    "\n",
    "#Move the dataset to GPU\n",
    "input_size = X_train.shape[1]\n",
    "X_train_t = torch.from_numpy(X_train).to(dtype = default_dtype)\n",
    "X_val_t   = torch.from_numpy(X_val  ).to(dtype = default_dtype)\n",
    "X_test_t  = torch.from_numpy(X_test ).to(dtype = default_dtype)\n",
    "Y_train_t = torch.from_numpy(Y_train).to(dtype = default_dtype)\n",
    "Y_val_t   = torch.from_numpy(Y_val  ).to(dtype = default_dtype)\n",
    "Y_test_t  = torch.from_numpy(Y_test ).to(dtype = default_dtype)\n",
    "\n",
    "if USE_CUDA and cuda_available:\n",
    "    X_train_t = X_train_t.cuda()\n",
    "    X_val_t   = X_val_t.cuda()\n",
    "    X_test_t  = X_test_t.cuda()\n",
    "    Y_train_t = Y_train_t.cuda()\n",
    "    Y_val_t   = Y_val_t.cuda()\n",
    "    Y_test_t  = Y_test_t.cuda()\n",
    "\n",
    "loss_functions = {\n",
    "    'SmoothL1': F.smooth_l1_loss,\n",
    "    'L1': F.l1_loss,\n",
    "    'MSE': F.mse_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b79b1-47dc-4079-83e7-a540655ddc67",
   "metadata": {},
   "source": [
    "# PyTorch training implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "10e77347-3df8-4c2f-8aea-ba31319f0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Initialize model weights\n",
    "def weights_init(layer_in):\n",
    "  if isinstance(layer_in, nn.Linear):\n",
    "    torch.nn.init.xavier_uniform_(layer_in.weight, gain=0.5)\n",
    "    layer_in.bias.data.fill_(0.0)\n",
    "\n",
    "## Turn data into sequences that can be used for training\n",
    "def get_batch_sequences(batch_idx, batch_size, permutation, X, Y):\n",
    "    sample_sz = X.shape[0]\n",
    "\n",
    "    batch_start = batch_idx * batch_size\n",
    "    batch_end   = min(sample_sz, (batch_idx + 1) * batch_size)\n",
    "    actual_size = batch_end - batch_start\n",
    "\n",
    "    batch_indeces = permutation[batch_start:batch_end]\n",
    "    batch_target = Y[batch_indeces]\n",
    "    batch_data = X[batch_indeces]\n",
    "\n",
    "    return [batch_data, batch_target, actual_size]\n",
    "\n",
    "## Training step\n",
    "def train(epoch, model, optimizer, scheduler, X, Y, batch_size, loss_fnc, print_mode):\n",
    "    model.train()\n",
    "\n",
    "    sample_sz = X.shape[0]\n",
    "    perm = np.random.permutation(sample_sz)\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch_idx in range(0, int(np.ceil(sample_sz / batch_size))):\n",
    "        batch_data, batch_target, actual_size = get_batch_sequences(batch_idx, batch_size, perm, X, Y)\n",
    "        \n",
    "        output = model(batch_data)\n",
    "        loss = loss_fnc(output, batch_target, reduction=\"mean\")\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print(optimizer.param_groups[0]['lr'])\n",
    "        if print_mode:\n",
    "            print('Epoch:{} \\tTrain loss: {:.7f}'.format(epoch, total_loss / (batch_idx + 1)), end=\"\\r\")\n",
    "            \n",
    "    total_loss /= np.ceil(sample_sz / batch_size)\n",
    "    if not print_mode:\n",
    "        print('Epoch:{} \\tTrain loss: {:.7f}'.format(epoch, total_loss), end=\"\\r\")\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "## Testing\n",
    "def test(model, X, Y, batch_size, loss_fnc):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    sample_sz = X.shape[0]\n",
    "    with torch.no_grad():\n",
    "        perm = np.random.permutation(sample_sz)\n",
    "        \n",
    "        for batch_idx in range(0, int(np.ceil(sample_sz / batch_size))):\n",
    "            batch_data, batch_target, actual_size = get_batch_sequences(batch_idx, batch_size, perm, X, Y)\n",
    "    \n",
    "            output = model(batch_data)\n",
    "            test_loss += loss_fnc(output, batch_target, reduction=\"mean\").item()\n",
    "\n",
    "    test_loss /= np.ceil(sample_sz / batch_size)\n",
    "    return test_loss\n",
    "\n",
    "def predict(model, X, batch_size):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    sample_sz = X.shape[0]\n",
    "    Y_dummy = np.zeros((sample_sz, 1))\n",
    "    Y_hat = np.zeros((sample_sz, 1))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        perm = np.arange(0, sample_sz)\n",
    "        \n",
    "        for batch_idx in range(0, int(np.ceil(sample_sz / batch_size))):\n",
    "            batch_data, batch_target, actual_size = get_batch_sequences(batch_idx, batch_size, perm, X, Y_dummy)\n",
    "    \n",
    "            output = model(batch_data)\n",
    "\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + actual_size\n",
    "                \n",
    "            Y_hat[start_idx:end_idx] = output[:].cpu()\n",
    "\n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c34b9c-5455-4274-9597-1a8514a84c2a",
   "metadata": {},
   "source": [
    "# Training and Grid search procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f3d59fb3-8161-41b0-83a7-f3977e7bbd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The whole training\n",
    "def train_model(model_class, params, print_mode):     \n",
    "    model = model_class(params)\n",
    "    loss_fnc = loss_functions[params[\"loss\"]]\n",
    "    epochs = params[\"epochs\"]\n",
    "    lr = params[\"lr\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    weight_decay = params[\"weight_decay\"]\n",
    "    \n",
    "    \n",
    "    if USE_CUDA and cuda_available:\n",
    "      model = model.cuda()\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get initial performance\n",
    "    test_loss = test(model, X_val_t, Y_val_t, batch_size, loss_functions[params[\"loss\"]])\n",
    "    if print_mode:\n",
    "        print('\\nInitial: Test loss: {:.7f}\\n'.format(test_loss))\n",
    "\n",
    "    # Train\n",
    "    train_loss = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(epochs * np.ceil(X_train_t.shape[0] / batch_size)), eta_min=0) #Cosine decay\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(epoch, model, optimizer, scheduler, X_train_t, Y_train_t, batch_size, loss_fnc, print_mode)\n",
    "\n",
    "        if print_mode:\n",
    "            # train_loss = test(model, X_train_t, Y_train_t, batch_size, loss_fnc)\n",
    "            val_loss   = test(model, X_val_t, Y_val_t, batch_size, loss_fnc)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            print('Epoch:' + str(epoch) + ' \\tTrain loss: {:.7f} \\tVal loss: {:.7f}'.format(train_loss, val_loss))\n",
    "            \n",
    "    final_val_loss = None\n",
    "    if not print_mode:\n",
    "        final_val_loss = test(model, X_val_t, Y_val_t, batch_size, loss_fnc)\n",
    "    else:\n",
    "        final_val_loss = val_losses[-1]\n",
    "    print(\"\\nTime to train: {:.3f}s\\t final val loss:{:.7f}\".format(time.time() - start_time, final_val_loss))\n",
    "    \n",
    "    if print_mode:\n",
    "        return [model, train_losses, val_losses]\n",
    "    else:\n",
    "        return [model, train_loss, final_val_loss]\n",
    "\n",
    "## Grid search\n",
    "def grid_search(model_class, param_variations):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_params = None\n",
    "\n",
    "    grid_val_losses = []\n",
    "    grid_train_losses = []\n",
    "    \n",
    "    for i in range(len(param_variations)):\n",
    "        params = param_variations[i]\n",
    "        print(\"\\nGrid search step {}/{}\".format(i + 1, len(param_variations)), \"\\t| params:\", params)\n",
    "        \n",
    "        model, train_loss, val_loss = train_model(model_class, params, print_mode=False)\n",
    "        \n",
    "        grid_val_losses.append(val_loss)\n",
    "        grid_train_losses.append(train_loss)\n",
    "        \n",
    "        if best_val_loss > val_loss:\n",
    "            print(\"New best!\")\n",
    "            best_val_loss = val_loss\n",
    "            best_params = params\n",
    "\n",
    "    print(\"\\nBest validation loss: \", best_val_loss)\n",
    "    print(\"Best params: \", best_params)\n",
    "    model, train_losses, val_losses = train_model(model_class, best_params, print_mode=True)\n",
    "    return [model, train_losses, val_losses, best_params, grid_val_losses, grid_train_losses]\n",
    "\n",
    "## Create list of parameter dictionaries for grid search\n",
    "def create_param_dict_from_lists(list_of_params, list_of_names):\n",
    "    params = []\n",
    "\n",
    "    feature_vals = list_of_params[0]\n",
    "    feature_name = list_of_names[0]\n",
    "    for feature_val in feature_vals:\n",
    "        params_row = {}\n",
    "        params_row[feature_name] = feature_val\n",
    "        params.append(params_row)\n",
    "    \n",
    "    for i in range(1, len(list_of_params)):\n",
    "        feature_vals = list_of_params[i]\n",
    "        feature_name = list_of_names[i]\n",
    "\n",
    "        old_params = params\n",
    "        new_params = []\n",
    "        for past_param in old_params:\n",
    "            for feature_val in feature_vals:\n",
    "                params_row = copy.copy(past_param)\n",
    "                params_row[feature_name] = feature_val\n",
    "                new_params.append(params_row)\n",
    "        params = new_params\n",
    "        \n",
    "    return params\n",
    "\n",
    "## Grid search plotting\n",
    "def plot_grid(grid_values, title, x_label, y_label, x_values, y_values):\n",
    "    plt.matshow(grid_values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(list(range(len(x_values))), x_values)\n",
    "    plt.yticks(list(range(len(y_values))), y_values)\n",
    "    \n",
    "    for (i, j), z in np.ndenumerate(grid_values):\n",
    "        plt.text(j, i, '{:0.5f}'.format(z), ha='center', va='center')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "## Plot training progress\n",
    "def plot_losses(train_losses, val_losses, title):\n",
    "    plt.title(title)\n",
    "    plt.plot(train_losses, label=\"train loss\")\n",
    "    plt.plot(val_losses, label=\"val loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "## Evaluation\n",
    "def evaluate_model(model, batch_size):\n",
    "    def print_metrics(model, X, Y, batch_size):\n",
    "        Y_hat = predict(model, X, batch_size)\n",
    "        \n",
    "        MSE = np.mean((Y_hat - Y) ** 2)\n",
    "        orig_MSE = np.mean((Y_scaler.inverse_transform(Y_hat) - Y_scaler.inverse_transform(Y)) ** 2)\n",
    "        print(\"\\tMSE:\", MSE)\n",
    "        print(\"\\tOriginal scale MSE:\", orig_MSE)\n",
    "        \n",
    "        print(\"\\n\\tRMSE:\", np.sqrt(MSE))\n",
    "        print(\"\\tOriginal scale RMSE:\", np.sqrt(orig_MSE))\n",
    "        \n",
    "        print(\"\\n\\tMAE:\", np.mean(np.abs(Y_hat - Y)))\n",
    "        print(\"\\tOriginal scale MAE:\", np.mean(np.abs(Y_scaler.inverse_transform(Y_hat) - Y_scaler.inverse_transform(Y))))\n",
    "\n",
    "    print(\"Train metrics:\")\n",
    "    print_metrics(model, X_train_t, Y_train, batch_size)\n",
    "    print(\"\\nValidation metrics:\")\n",
    "    print_metrics(model, X_val_t, Y_val, batch_size)\n",
    "    print(\"\\nTest metrics:\")\n",
    "    print_metrics(model, X_test_t, Y_test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5b2c7a-5207-41cc-b768-39ad77803847",
   "metadata": {},
   "source": [
    "# PyTorch MLP implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8e9e7a7-6aa8-422c-b941-e8f586b75083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All grid searched values:\n",
      "\n",
      "{'epochs': 200, 'batch_size': 20, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': None, 'hidden_dropout_rate': None, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 20, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': None, 'hidden_dropout_rate': 0.5, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 20, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': None, 'hidden_dropout_rate': 0.95, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 20, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': 0.5, 'hidden_dropout_rate': None, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 20, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': 0.5, 'hidden_dropout_rate': 0.5, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 20, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': 0.5, 'hidden_dropout_rate': 0.95, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 20, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': 0.95, 'hidden_dropout_rate': None, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 20, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': 0.95, 'hidden_dropout_rate': 0.5, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 20, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': 0.95, 'hidden_dropout_rate': 0.95, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 50, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': None, 'hidden_dropout_rate': None, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 50, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': None, 'hidden_dropout_rate': 0.5, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 50, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': None, 'hidden_dropout_rate': 0.95, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 50, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': 0.5, 'hidden_dropout_rate': None, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 50, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': 0.5, 'hidden_dropout_rate': 0.5, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 50, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': 0.5, 'hidden_dropout_rate': 0.95, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 50, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': 0.95, 'hidden_dropout_rate': None, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 50, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': 0.95, 'hidden_dropout_rate': 0.5, 'hidden_size': 10000}\n",
      "{'epochs': 200, 'batch_size': 50, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': 0.95, 'hidden_dropout_rate': 0.95, 'hidden_size': 10000}\n",
      "\n",
      "Grid search step 1/18 \t| params: {'epochs': 200, 'batch_size': 20, 'lr': 1e-05, 'loss': 'MSE', 'weight_decay': 0, 'input_dropout_rate': None, 'hidden_dropout_rate': None, 'hidden_size': 10000}\n",
      "Epoch:95 \tTrain loss: 0.0360941\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(param)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Grid search\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m model_MLP, train_losses_MLP, val_losses_MLP \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMLP_Regressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m plot_losses(train_losses_MLP, val_losses_MLP, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLP Regressor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m evaluate_model(model_MLP, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[60], line 59\u001b[0m, in \u001b[0;36mgrid_search\u001b[1;34m(model_class, param_variations)\u001b[0m\n\u001b[0;32m     57\u001b[0m params \u001b[38;5;241m=\u001b[39m param_variations[i]\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGrid search step \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(param_variations)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m| params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, params)\n\u001b[1;32m---> 59\u001b[0m model, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_val_loss \u001b[38;5;241m>\u001b[39m val_loss:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew best!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[60], line 30\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model_class, params, print_mode)\u001b[0m\n\u001b[0;32m     28\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m lr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(epochs \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mceil(X_train_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m batch_size)), eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#Cosine decay\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fnc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m print_mode:\n\u001b[0;32m     33\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m test(model, X_train_t, Y_train_t, batch_size, loss_fnc)\n",
      "Cell \u001b[1;32mIn[59], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch, model, optimizer, scheduler, X, Y, batch_size, loss_fnc, print_mode)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(sample_sz \u001b[38;5;241m/\u001b[39m batch_size))):\n\u001b[0;32m     30\u001b[0m     batch_data, batch_target, actual_size \u001b[38;5;241m=\u001b[39m get_batch_sequences(batch_idx, batch_size, perm, X, Y)\n\u001b[1;32m---> 32\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fnc(output, batch_target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FinTech\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FinTech\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[65], line 25\u001b[0m, in \u001b[0;36mMLP_Regressor.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dropout_rate \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dropout(x)\n\u001b[1;32m---> 25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FinTech\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FinTech\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\FinTech\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class MLP_Regressor(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(MLP_Regressor, self).__init__()\n",
    "\n",
    "        self.hidden_size = params[\"hidden_size\"]\n",
    "        self.input_dropout_rate = params[\"input_dropout_rate\"]\n",
    "        self.hidden_dropout_rate = params[\"hidden_dropout_rate\"]\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, self.hidden_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "        if self.input_dropout_rate != None:\n",
    "            self.input_dropout = nn.Dropout(self.input_dropout_rate)\n",
    "        if self.hidden_dropout_rate != None:\n",
    "            self.hidden_dropout = nn.Dropout(self.hidden_dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.input_dropout_rate != None:\n",
    "            x = self.input_dropout(x)\n",
    "            \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        \n",
    "        if self.hidden_dropout_rate != None:\n",
    "            x = self.hidden_dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Parameters for grid search\n",
    "epochs = [200]\n",
    "batch_size = [50]\n",
    "lrs = [1e-5]\n",
    "loss = [\"MSE\"]\n",
    "weight_decay = [0]\n",
    "\n",
    "input_dropout_rate = [None, 0.5, 0.95]\n",
    "hidden_dropout_rate = [None, 0.5, 0.95]\n",
    "hidden_size = [10000]\n",
    "\n",
    "# Parameter formatting\n",
    "all_param_vals = [epochs, batch_size, lrs, loss, weight_decay, input_dropout_rate, hidden_dropout_rate, hidden_size]\n",
    "feature_names = [\"epochs\", \"batch_size\", \"lr\", \"loss\", \"weight_decay\", \"input_dropout_rate\", \"hidden_dropout_rate\", \"hidden_size\"]\n",
    "params = create_param_dict_from_lists(all_param_vals, feature_names)\n",
    "\n",
    "print(\"All grid searched values:\\n\")\n",
    "for param in params:\n",
    "    print(param)\n",
    "\n",
    "# Grid search\n",
    "model_MLP, train_losses_MLP, val_losses_MLP, best_params, grid_val_losses, grid_train_losses = grid_search(MLP_Regressor, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a2c39-865d-4602-a79f-557031f43a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show grid search progress\n",
    "# train_grid = np.zeros((len(lrs), len(batch_size)))\n",
    "# val_grid = np.zeros((len(lrs), len(batch_size)))\n",
    "\n",
    "# for i in range(len(grid_val_losses)):\n",
    "#     train_grid[i % len(lrs), i // len(lrs)] = grid_train_losses[i]\n",
    "#     val_grid[i % len(lrs), i // len(lrs)] = grid_val_losses[i]\n",
    "    \n",
    "# plot_grid(grid_values=train_grid, title=\"MLP MSE Grid search val losses\", x_label=\"batch_size\", y_label=\"learning rate\", x_values=batch_size, y_values=lrs)\n",
    "# plot_grid(grid_values=val_grid, title=\"MLP MSE Grid search train losses\", x_label=\"batch_size\", y_label=\"learning rate\", x_values=batch_size, y_values=lrs)\n",
    "\n",
    "# Plot the training progress\n",
    "plot_losses(train_losses_MLP, val_losses_MLP, \"MLP Regressor\")\n",
    "\n",
    "# Show final model evaluation\n",
    "print(\"best_params: \", best_params)\n",
    "evaluate_model(model_MLP, batch_size=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
